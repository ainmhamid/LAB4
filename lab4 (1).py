# -*- coding: utf-8 -*-
"""LAB4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G4az7FwvsuAeACFRDQC6VewqE4Q7UnJB
"""

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn

st.set_page_config(
    page_title="Activation Function Visualisation",
    layout="centered"
)

st.title("Activation Function Visualisation")
st.write(
    """
    This web application visualises common **activation functions**
    used in neural networks:
    - ReLU
    - Sigmoid
    - Tanh
    """
)

st.sidebar.header("Settings")

activation_type = st.sidebar.selectbox(
    "Choose Activation Function",
    ["ReLU", "Sigmoid", "Tanh"]
)

x_min = st.sidebar.slider("Minimum x value", -10.0, 0.0, -5.0)
x_max = st.sidebar.slider("Maximum x value", 0.0, 10.0, 5.0)
num_points = st.sidebar.slider("Number of points", 100, 2000, 500)

x = np.linspace(x_min, x_max, num_points)
x_tensor = torch.from_numpy(x).float()

if activation_type == "ReLU":
    activation = nn.ReLU()
    y = activation(x_tensor).numpy()
    formula = r"$f(x) = \max(0, x)$"
    y_label = "ReLU(x)"

elif activation_type == "Sigmoid":
    activation = nn.Sigmoid()
    y = activation(x_tensor).numpy()
    formula = r"$f(x) = \frac{1}{1 + e^{-x}}$"
    y_label = "Sigmoid(x)"

else:  # Tanh
    activation = nn.Tanh()
    y = activation(x_tensor).numpy()
    formula = r"$f(x) = \tanh(x)$"
    y_label = "Tanh(x)"

st.subheader(f"{activation_type} Function")
st.latex(formula)

fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_xlabel("x")
ax.set_ylabel(y_label)
ax.set_title(f"{activation_type} Activation Function")
ax.axhline(0)
ax.axvline(0)

st.pyplot(fig)